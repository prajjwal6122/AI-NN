\documentclass[10pt,a4paper,twoside]{article}
\usepackage[dutch]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{float,flafter}	
\usepackage{hyperref}
\usepackage{inputenc}
\setlength\paperwidth{20.999cm}\setlength\paperheight{29.699cm}\setlength\voffset{-1in}\setlength\hoffset{-1in}\setlength\topmargin{.1cm}\setlength\headheight{12pt}\setlength\headsep{0cm}\setlength\footskip{1.131cm}\setlength\textheight{26cm}\setlength\oddsidemargin{2.499cm}\setlength\textwidth{15.999cm}

\begin{document}
\begin{center}
\hrule

\vspace{.4cm}
{\bf {\Large ASSIGNMENT-2 }}\\
\vspace{.3cm}
{\bf {\huge Hidden Markov Model}}
\vspace{.3cm}
\end{center}
{\bf Name:}  BINAY KUMAR SETHI\\
{\bf Roll no:}  19111021 \\
{\bf Branch: }  Biomedical Engineering \hspace{\fill}  22 July, 2021 \\
\hrule

\vspace{.3cm}
\section{Hidden Markov Model:} 
Hidden Markov Model is a statistical Markov model in which the system being modeled is assumed to be a Markov process – call it X – with unobservable states. HMM assumes that there is another process Y whose behavior "depends" on X. The goal is to learn about X by observing Y. 

\section{Advantages:}
Hidden Markov Model has a strong statistical foundation with efficient learning algorithms where learning can take place directly from raw sequence data. It allows consistent treatment of insertion and deletion penalties in the form of locally learnable methods and can handle inputs of variable length. 

\section{Major Components:}
A Hidden Markov Model consists of two components. Each HMM contains a series of discrete-state, time-homologous, first-order Markov chains (MC) with suitable transition probabilities between states and an initial distribution.

\section{Examples:}
{\bf Drawing balls from hidden urns}\\
In its discrete form, a hidden Markov process are often visualized as a generalization of the urn problem with replacement where each item from the urn is returned to the first urn before subsequent step. The Markov process itself can't be observed, only the sequence of labelled balls, thus this arrangement is named a hidden Markov process. \\
\\
{\bf Weather Guessing Game}\\
Consider two friends, Alice and Bob, who live far aside from one another and who talk together daily over the phonephone about what they did that day. Bob is merely curious about three activities: walking within the park, shopping, and cleaning his apartment. the selection of what to try to to is decided exclusively by the weather on a given day. Alice has no definite information about the weather, but she knows general trends. supported what Bob tells her he did every day , Alice tries to guess what the weather must are like.
Alice believes that the weather operates as a discrete Markov chain . There are two states, "Rainy" and "Sunny", but she cannot observe them directly, that is, they're hidden from her. On every day , there's a particular chance that Bob will perform one among the subsequent activities, counting on the weather: "walk", "shop", or "clean". Since Bob tells Alice about his activities, those are the observations. the whole system is that of a hidden Markov model (HMM).\\

\section{Inference:}
{\bf Probability of an observed sequence:}
The task is to compute in a best way, given the parameters of the model, the probability of a particular output sequence. \\
{\bf Probability of the latent variables:}
A number of related tasks ask about the probability of one or more of the latent variables, given the model's parameters and a sequence of observations y(1),....,y(t).
{\bf Statistical significance:}For some of the above problems, it's going to even be interesting to ask about statistical significance. what's the probability that a sequence drawn from some null distribution will have an HMM probability or a maximum state sequence probability a minimum of as large as that of a specific output sequence? When an HMM is employed to guage the relevance of a hypothesis for a specific output sequence, the statistical significance indicates the false positive rate related to failing to reject the hypothesis for the output sequence.




\section{Applications:}
Hidden Markov Model can be applied in many fields where the goal is to recover a data sequence that is not immediately observable. Applications include: Computational finance, Single-molecule kinetic analysis, Cryptanalysis Speech recognition, including Siri, Speech synthesis, Part-of-speech tagging, Document separation in scanning solutions Machine translation, Partial discharge, Gene prediction, Handwriting recognition, Alignment of bio-sequences, Time series analysis, Activity recognition, Protein folding, Sequence classification, Metamorphic virus detection, DNA motif discovery, DNA hybridization, kinetics, Chromatin state discovery,Transportation forecasting Solar irradiance variability 




\end{document}
